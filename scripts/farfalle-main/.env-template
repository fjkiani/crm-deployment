# Add provider keys for intelligence enrichment
TAVILY_API_KEY=
DIFFBOT_TOKEN=
LINKEDIN_RAPIDAPI_KEY=
LINKEDIN_RAPIDAPI_HOST=linkedin-data-api.p.rapidapi.com
GEMINI_API_KEY=
# Optional: override LiteLLM model ids for Gemini variants
GEMINI_1_5_MODEL=gemini/gemini-1.5-pro
GEMINI_2_5_MODEL=gemini/gemini-2.5-pro
# 1 - Search
# Options: searxng, tavily, serper, bing
SEARCH_PROVIDER=searxng
SEARXNG_BASE_URL=http://searxng:8080

# tavily, serper, bing (Optional)
TAVILY_API_KEY=
SERPER_API_KEY=
BING_API_KEY=

# 2 - LLMs
OLLAMA_API_BASE=http://host.docker.internal:11434

# Cloud Models (Optional)
OPENAI_API_KEY=
OPENAI_API_BASE=
GROQ_API_KEY=

AZURE_DEPLOYMENT_NAME=
AZURE_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=

# azure, openai
OPENAI_MODE=openai

# Any `provider/model` from https://litellm.vercel.app/docs/providers
CUSTOM_MODEL=

# 3 - Frontend
NEXT_PUBLIC_API_URL=http://localhost:8000

# DB
DATABASE_URL=postgresql+psycopg2://postgres:password@db:5432/postgres
DB_ENABLED=True


# 4 - Caching + Rate Limiting (Optional)
RATE_LIMIT_ENABLED=False
REDIS_URL=


# 5 - Local Models
ENABLE_LOCAL_MODELS=True
NEXT_PUBLIC_LOCAL_MODE_ENABLED=True
NEXT_PUBLIC_PRO_MODE_ENABLED=True
